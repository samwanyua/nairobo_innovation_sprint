{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f0c616",
   "metadata": {},
   "source": [
    "# Inference 101 using Whisper Models from HuggingFace\n",
    "\n",
    "Note: there are many different ways to run inference. This is just one example to demonstrate how the audio data from the datasets can be run through a model.\n",
    "\n",
    "We are focussing on Whisper models here exclusively, but there are other models that one could use. More to this later..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e5e42",
   "metadata": {},
   "source": [
    "## Preparation -- Imports and Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from huggingface_hub import hf_hub_download\n",
    "from IPython.display import Audio, display\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a902946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, whoami\n",
    "HF_TOKEN = input()\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cdli/kenyan_english_nonstandard_speech_v0'\n",
    "ds = datasets.load_dataset(dataset_name, split='test', streaming=False)\n",
    "ds = ds.filter(lambda example: example['audio_length'] <= 30)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e127df",
   "metadata": {},
   "source": [
    "## Load a model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHISPER_MODEL_NAME = \"openai/whisper-tiny\"\n",
    "# WHISPER_MODEL_NAME = \"openai/whisper-small\"\n",
    "# WHISPER_MODEL_NAME = \"openai/whisper-large-v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6e0f47",
   "metadata": {},
   "source": [
    "### Easiest way is via HF's pipeline approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"automatic-speech-recognition\", \n",
    "                model=WHISPER_MODEL_NAME,\n",
    "                #return_timestamps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2f41e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs={\n",
    "    \"language\": 'en', \n",
    "    \"task\": \"transcribe\",\n",
    "    \"max_length\": 448, # Note: don't exceed 448 - otherwise you'll get index errors when max_length exceeds the models positional encoding limits\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b399ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = ds[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(example['audio']['array'], rate=example['audio']['sampling_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipe(example['audio']['array'])\n",
    "prediction['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce915f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "example['transcription']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a6937",
   "metadata": {},
   "source": [
    "### Separate Processor and Model Output Generation\n",
    "\n",
    "* not the recommended approach if what you want are predictions\n",
    "* but can be handy if you care to analyze the intermediate representations or want to look deeper into the predicted IDs or just want to learn how the model works internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262f5c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import torch\n",
    "\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(WHISPER_MODEL_NAME)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(WHISPER_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(example['audio']['array'], \n",
    "                   sampling_rate=example['audio']['sampling_rate'], \n",
    "                   return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        inputs[\"input_features\"],\n",
    "        # language=\"en\",\n",
    "        # task=\"transcribe\",\n",
    "        # max_length=448,  # Whisper's max length - do not exceed!\n",
    "        num_beams=1,\n",
    "        #temperature=0.7,\n",
    "        #do_sample=True\n",
    "        do_sample=False\n",
    "        )\n",
    "\n",
    "result = processor.batch_decode(generated_ids)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08a65c",
   "metadata": {},
   "source": [
    "## Next step\n",
    "\n",
    "* try different model sizes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
