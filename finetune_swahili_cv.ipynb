{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21c1499",
   "metadata": {},
   "source": [
    "# Fine-tune Whisper for Swahili\n",
    "\n",
    "* using small Common Voice Data: https://huggingface.co/datasets/cdli/common_voice_swahili_small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f87c76",
   "metadata": {},
   "source": [
    "## Settings \n",
    "\n",
    "--> adapt for your scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43222608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "HF_TOKEN = input()\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3ce04",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aac5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# storage in Volume that will persist\n",
    "LOCAL_STORAGE_DIR = '/jupyter_kernel'\n",
    "\n",
    "BASE_DIR = os.path.join(LOCAL_STORAGE_DIR, 'trained_models')\n",
    "!mkdir -p {BASE_DIR}\n",
    "\n",
    "# directory for model training\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'sw_cv_tune_whisper_small_1')\n",
    "# OUTPUT_DIR = os.path.join(BASE_DIR, 'sw_train_with_cv_whisper_small_1')\n",
    "# OUTPUT_DIR = os.path.join(BASE_DIR, 'sw_train_with_cv_whisper_largev3_1')\n",
    "\n",
    "print(f\"Will write model to: {OUTPUT_DIR}\")\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    raise ValueError(f\"Output directory already exists - if you continue this will overwrite data and may lead to strange results...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341cb64",
   "metadata": {},
   "source": [
    "### Model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHISPER_MODEL_TYPE = \"openai/whisper-tiny\" \n",
    "WHISPER_MODEL_TYPE = \"openai/whisper-small\" \n",
    "# WHISPER_MODEL_TYPE = \"openai/whisper-large-v3\" \n",
    "\n",
    "LANGUAGE = 'sw'\n",
    "TASK = \"transcribe\"\n",
    "\n",
    "# which parts of the model to update\n",
    "UPDATE_ENCODER = True\n",
    "UPDATE_DECODER = True\n",
    "UPDATE_PROJ = True\n",
    "\n",
    "#################\n",
    "## Base Model\n",
    "#################\n",
    "\n",
    "BASE_MODEL_NAME = WHISPER_MODEL_TYPE\n",
    "print('Base model will be loaded from:', BASE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2735de",
   "metadata": {},
   "source": [
    "### Trainer Settings\n",
    "\n",
    "--> adjust as needed or keep defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104baa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOGGING_STEPS = 5\n",
    "# if save steps is 0, only last and best model will be written\n",
    "SAVE_STEPS = 50\n",
    "\n",
    "# training duration\n",
    "MAX_EPOCHS = 5\n",
    "MAX_STEPS = 1000  # for larger datasets, you will want to increase this\n",
    "\n",
    "# Learning Rate and LR Scheduler (LR_END and LR_DECAY_POWER only apply to polynomial)\n",
    "LEARNING_RATE = 1e-4 #@param\n",
    "LR_SCHEDULER_TYPE = 'polynomial' # constant_with_warmup or polynomial\n",
    "LR_WARMUP_STEPS = 100\n",
    "LR_END = 1e-8\n",
    "LR_DECAY_POWER = 4\n",
    "# see: https://huggingface.co/docs/transformers/v4.46.2/en/main_classes/optimizer_schedules#transformers.SchedulerType\n",
    "# and here: https://www.kaggle.com/code/snnclsr/learning-rate-schedulers\n",
    "# constant --> 'constant_with_warmup'\n",
    "# polynomial --> 'get_polynomial_decay_schedule_with_warmup'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 16\n",
    "\n",
    "#@markdown other settings relevant for evaluation\n",
    "MAX_GEN_LEN = 128 # increase if your data has long sequences!\n",
    "EVAL_ON_START = True\n",
    "EVAL_STEPS = 50\n",
    "\n",
    "# for CPU, set both to false\n",
    "USE_FP16 = True\n",
    "USE_BF16 = False # only some GPUs support this, eg A100, A40\n",
    "\n",
    "# checkpoints get huge for large models (~18 GB!)\n",
    "NUM_CHECKPOINTS_TO_STORE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57078735",
   "metadata": {},
   "source": [
    "## Imports and Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# more efficient dataset handling\n",
    "datasets.disable_caching()\n",
    "print('cache:', datasets.is_caching_enabled())\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "torch.get_num_threads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3acd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have gpu\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7034f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import random\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "\n",
    "import tarfile\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import evaluate\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "transcript_normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903eac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wer(references, predictions, normalize=True, verbose=True):\n",
    "  rs = references\n",
    "  ps = predictions\n",
    "  if normalize:\n",
    "    ps = [transcript_normalizer(x) for x in predictions]\n",
    "    rs = [transcript_normalizer(x) for x in references]\n",
    "  if verbose:\n",
    "    for r, p in zip(rs, ps):\n",
    "      print(r)\n",
    "      print(p)\n",
    "      print()\n",
    "\n",
    "  return wer_metric.compute(references=rs, predictions=ps)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # for training metrics\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_strs = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_strs = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # calculate a per-example average\n",
    "    wers = []\n",
    "    cers = []\n",
    "    for pred_str, label_str in zip(pred_strs, label_strs):\n",
    "      p = transcript_normalizer(pred_str)\n",
    "      l = transcript_normalizer(label_str)\n",
    "      wer = wer_metric.compute(predictions=[p], references=[l])\n",
    "      cer = cer_metric.compute(predictions=[p], references=[l])\n",
    "      wers.append(wer)\n",
    "      cers.append(cer)\n",
    "\n",
    "    wer = np.mean([min(1.0,x) for x in wers])\n",
    "    cer = np.mean([min(1.0,x) for x in cers])\n",
    "    print('adjusted:', wer, cer)\n",
    "    print('un-adjusted:', np.mean(wers), np.mean(cers))\n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d913fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name, limit_to_30_seconds=True):\n",
    "    \"\"\"\n",
    "    Load a dataset from Hugging Face Hub.\n",
    "    If limit_to_30_seconds is True, will only load examples with audio length <= 30 seconds.\n",
    "    \"\"\"\n",
    "    ds = datasets.load_dataset(dataset_name, split='test', streaming=False)\n",
    "    orig_len = len(ds)\n",
    "    if limit_to_30_seconds:\n",
    "        ds = ds.filter(lambda example: example['audio_length'] <= 30)\n",
    "        print(f\"Filtered dataset from {orig_len} to {len(ds)} examples with audio length <= 30 seconds\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following warning can be ignored:\n",
    "# \"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\"\n",
    "# See: https://discuss.huggingface.co/t/finetuning-whisper-attention-mask-not-set-and-canot-be-inferred/97456\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb8215",
   "metadata": {},
   "source": [
    "## Download datasets and prepare features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fcf0e7",
   "metadata": {},
   "source": [
    "### Optimizing some settings for dataset access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.disable_caching()\n",
    "print('cache:', datasets.is_caching_enabled())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device is: ', device)\n",
    "\n",
    "# IMPORTANT! need to set to 1 to avoid the mapping to hang!\n",
    "torch.set_num_threads(1)\n",
    "torch.get_num_threads()\n",
    "\n",
    "num_proc = min(32, os.cpu_count())\n",
    "print('# processors:', num_proc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a27923",
   "metadata": {},
   "source": [
    "### Load feature extractor\n",
    "\n",
    "--> for the model type you specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85919d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load processor\n",
    "print('Using Language: ', LANGUAGE)\n",
    "print('Using model:', WHISPER_MODEL_TYPE)\n",
    "processor = WhisperProcessor.from_pretrained(WHISPER_MODEL_TYPE, language=LANGUAGE, task=TASK)\n",
    "\n",
    "# since this tokenizer isn't a FastTokenizer, so there is no point in running it with is_batched=True\n",
    "# see: processor.tokenizer.is_fast\n",
    "def prepare_features(example):\n",
    "    example[\"input_features\"] = processor.feature_extractor(example[\"audio\"][\"array\"], sampling_rate=example[\"audio\"][\"sampling_rate\"]).input_features[0]\n",
    "    example[\"labels\"] = processor.tokenizer(example[\"transcription\"]).input_ids\n",
    "    # also count number of tokens\n",
    "    example[\"token_length\"] = len(example[\"labels\"])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d19cd",
   "metadata": {},
   "source": [
    "### Load and prepare Swahili Common Voice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a020684",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ds = datasets.load_dataset(\"cdli/common_voice_swahili_small\", streaming=False)\n",
    "cv_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a95a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = cv_ds['train'].shuffle(seed=42).flatten_indices()\n",
    "ds_train = ds_train.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "\n",
    "print(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34fa18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dev = cv_ds['validation'].shuffle(seed=42).flatten_indices()\n",
    "ds_dev = ds_dev.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "\n",
    "print(ds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = cv_ds['test'].shuffle(seed=42).flatten_indices()\n",
    "ds_test = ds_test.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "\n",
    "print(ds_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d1182",
   "metadata": {},
   "source": [
    "## Prepare Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)\n",
    "_ = base_model.to(device)\n",
    "print('Using Language: ', LANGUAGE)\n",
    "print('Using model:', WHISPER_MODEL_TYPE)\n",
    "\n",
    "# ensure task and language for training\n",
    "base_model.generation_config.language = LANGUAGE\n",
    "base_model.generation_config.task = TASK\n",
    "base_model.generation_config.forced_decoder_ids = None\n",
    "base_model.config.forced_decoder_ids = None\n",
    "# to use gradient checkpointing\n",
    "base_model.config.use_cache = False\n",
    "print('language set to:', base_model.generation_config.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which layers to tune\n",
    "base_model.model.encoder.requires_grad_(UPDATE_ENCODER)\n",
    "base_model.model.decoder.requires_grad_(UPDATE_DECODER)\n",
    "base_model.proj_out.requires_grad_(UPDATE_PROJ)\n",
    "\n",
    "print(\"Overview to number of model parameters to be updated:\")\n",
    "print('* encoder params to update/total:', count_trainable_parameters(base_model.model.encoder), base_model.model.encoder.num_parameters())\n",
    "print('* decoder parans to update/total:', count_trainable_parameters(base_model.model.decoder), base_model.model.decoder.num_parameters())\n",
    "\n",
    "print('* overall # trainable parameters:', count_trainable_parameters(base_model))\n",
    "print('*     overall # model parameters:', base_model.model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d40350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyper Parameters\n",
    "# don't change settings here, but instead at very top!\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, 'logs'),\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    include_num_input_tokens_seen=True,\n",
    "    ### on GPU, can either do fp16 or bf16 depending on specific GPU\n",
    "    fp16=USE_FP16, \n",
    "    bf16=USE_BF16, \n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    #\n",
    "    num_train_epochs=MAX_EPOCHS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    #\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    #\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    #\n",
    "    eval_on_start=EVAL_ON_START,\n",
    "    predict_with_generate=True,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    eval_strategy=\"steps\",\n",
    "    generation_max_length=MAX_GEN_LEN,\n",
    "    #\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    #\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    #\n",
    "    # only applies to polynomial schedule (constant ignores args)\n",
    "    lr_scheduler_kwargs={\n",
    "        \"lr_end\": LR_END, # The final LR.  Crucial for polynomial decay.\n",
    "        \"power\": LR_DECAY_POWER, # for decay\n",
    "        # we don't need to set the other arguments as they are already set in the args outside\n",
    "        #\"num_warmup_steps\": WARMUP_STEPS, # The number of steps for the warmup phase.\n",
    "        #\"num_training_steps\": MAX_STEPS, # The total number of training steps.\n",
    "        #\"lr_init\": 1e-5 # we take the LR setting\n",
    "    },\n",
    "\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=LR_WARMUP_STEPS, # what happens if we have this and the LR schedule args ?\n",
    "    #\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=NUM_CHECKPOINTS_TO_STORE,\n",
    "    load_best_model_at_end=True,\n",
    "    # group_by_length=True\n",
    "    # auto_find_batch_size=True\n",
    ")\n",
    "\n",
    "print('trainer args set, writing to:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39301594",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=base_model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=base_model,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_dev,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244a087",
   "metadata": {},
   "source": [
    "## Run the training\n",
    "\n",
    "Note: tensorboard doesn't show properly in jupyter notebooks, use the tensorboard_server.py tool to host a tensorboard instance on Modal, using below model training dir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5fb010",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model training dir:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740407b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train from scratch\n",
    "trainer.train()\n",
    "\n",
    "# # alternatively, you can continue training if a previous job was interrupted\n",
    "# trainer.train(resume_from_checkpoint = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8842a",
   "metadata": {},
   "source": [
    "## Post-Training Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2780af0",
   "metadata": {},
   "source": [
    "### On Swahili CV dev-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeecbc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on dev-set \n",
    "# (should give the same result shown in trainig progress on dev set)\n",
    "trainer.evaluate(ds_dev, language=LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5f647",
   "metadata": {},
   "source": [
    "### On Swahili CV test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807deace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on test-set \n",
    "# (should give the same result shown in trainig progress on dev set)\n",
    "trainer.evaluate(ds_test, language=LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca11df34",
   "metadata": {},
   "source": [
    "### On Swahili non-standard speech test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a14759",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_nss = load_dataset(\"cdli/kenyan_swahili_nonstandard_speech_v0\", limit_to_30_seconds=True)\n",
    "ds_test_nss = ds_test_nss.map(prepare_features, remove_columns=['audio'], writer_batch_size=1, num_proc=num_proc)\n",
    "ds_test_nss\n",
    "print(f\"Loaded dataset with {len(ds_test_nss)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(ds_test_nss.take(100), language=LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9082ef",
   "metadata": {},
   "source": [
    "## Store Model\n",
    "\n",
    "--> save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff87bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with \"load_best_model_at_end=True\" set in the settings (this is the default, so don't change that), after training is completed the best model is loaded and then saved\n",
    "best_model_dir = os.path.join(OUTPUT_DIR, 'best_model')\n",
    "print(f\"Saving to: {best_model_dir}\")\n",
    "trainer.model.save_pretrained(best_model_dir, safe_serialization=True)\n",
    "trainer.tokenizer.save_pretrained(best_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d356636",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
